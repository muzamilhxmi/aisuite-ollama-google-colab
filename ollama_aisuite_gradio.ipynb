{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JYpRacYQH_9S",
        "ocdBfrNuIHyR",
        "uvpeGC7gU2SC",
        "kkA4AFJiVFmc"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Install ollama using shell, no need to use xterm or something!**"
      ],
      "metadata": {
        "id": "JYpRacYQH_9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "collapsed": true,
        "id": "E696GihxKL1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Make sure the system NVIDIA libraries and CUDA libraries are preferred\n",
        "os.environ.update({\n",
        "    'LD_LIBRARY_PATH': '/usr/lib64-nvidia:/usr/local/cuda/lib64'\n",
        "})\n"
      ],
      "metadata": {
        "id": "MGkZ6lXcKLxV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IuF09OViKuoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NLsUnOFdKuip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Install aisuite and run models you like to test**"
      ],
      "metadata": {
        "id": "ocdBfrNuIHyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install AI Suite\n",
        "!pip install aisuite[ollama]\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "1mt8kgFHXMvv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import aisuite as ai\n",
        "\n",
        "# Initialize the AI client for accessing the language model\n",
        "def ask(message, sys_message=\"You are a helpful agent.\", model=\"ollama:llama3.1:8b\"):\n",
        "    # Initialize the AI client for accessing the language model\n",
        "    client = ai.Client()\n",
        "\n",
        "    # Construct the messages list for the chat\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": sys_message},\n",
        "        {\"role\": \"user\", \"content\": message}\n",
        "    ]\n",
        "\n",
        "    # Send the messages to the model and get the response\n",
        "    response = client.chat.completions.create(model=model, messages=messages)\n",
        "\n",
        "    # Return the content of the model's response\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "n8DK8_RqqXFH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3.2:1b\n",
        "!ollama pull phi3\n",
        "!ollama pull smollm2:135m\n",
        "!ollama pull tinyllama:latest\n",
        "!ollama pull gemma2:2b\n",
        "\n",
        "models = [\n",
        "  'llama3.2:1b',\n",
        "  'smollm2:135m',\n",
        "  'phi3',\n",
        "  'tinyllama:latest',\n",
        "  'gemma2:2b'\n",
        "]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FMFNM5hbN2Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Raw output as simple text**"
      ],
      "metadata": {
        "id": "uvpeGC7gU2SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to hold the responses from each model\n",
        "res = []\n",
        "\n",
        "# Loop through each model and get a response for the specified question\n",
        "for x in models:\n",
        "    res.append(ask('Write a short one sentence explanation of the origins of AI?', model=f'ollama:{x}'))\n",
        "\n",
        "# Print the model's name and its corresponding response\n",
        "for idx, x in enumerate(res):\n",
        "    print(models[idx] + ': \\n ' + x + ' ' + '\\n\\n')"
      ],
      "metadata": {
        "id": "E_gg-sgYuoOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Cleaner output using Gradio**"
      ],
      "metadata": {
        "id": "kkA4AFJiVFmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "models = [\n",
        "    'llama3.2:1b',\n",
        "    'smollm2:135m',\n",
        "    'phi3',\n",
        "    'tinyllama:latest',\n",
        "    'gemma2:2b'\n",
        "]\n",
        "\n",
        "def get_model_response(model):\n",
        "    # Replace this with your actual function call, e.g.:\n",
        "    # return ask('Write a short one sentence explanation of the origins of AI?', model=f'ollama:{model}')\n",
        "    return ask('Write a short one sentence explanation of the origins of AI?', model=f'ollama:{model}')\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Model Responses\")\n",
        "    with gr.Tabs():\n",
        "        for model in models:\n",
        "            with gr.TabItem(label=model):\n",
        "                response = get_model_response(model)\n",
        "                gr.Markdown(response)\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "leNkmA13M5Te"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}